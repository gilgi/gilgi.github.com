<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gilgi.org (Posts about Dota 2)</title><link>https://gilgi.org/</link><description></description><atom:link href="https://gilgi.org/categories/dota-2.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:site@gilgi.org"&gt;gilgi&lt;/a&gt; </copyright><lastBuildDate>Tue, 17 Dec 2019 22:57:05 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Dota 2 friend network visualization</title><link>https://gilgi.org/blog/dotafriends/</link><dc:creator>gilgi</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img src="https://gilgi.org/images/blog/dotafriends.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;What better way is there to enjoy games than with friends? Of course, some friends have better synergies than others. What if there was a way to quantify those synergies and draw a group of friends as a graph, with high-synergy friends placed close together and low-synergy friends placed further apart?&lt;/p&gt;
&lt;p&gt;In this post, we'll use the &lt;a href="https://docs.opendota.com/"&gt;OpenDota API&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Multidimensional_scaling"&gt;multidimensional scaling (MDS)&lt;/a&gt;, and &lt;a href="https://networkx.github.io/"&gt;networkx&lt;/a&gt; to visualize a network of friends playing Dota 2 together.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1lJkUGqKbb8GSzYP6ClreBx41lbwtf1P2"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gilgi.org/blog/dotafriends/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>API</category><category>Dota 2</category><category>graph</category><category>MDS</category><category>notebook</category><category>Python</category><category>visualization</category><guid>https://gilgi.org/blog/dotafriends/</guid><pubDate>Wed, 14 Aug 2019 04:00:00 GMT</pubDate></item><item><title>Visualizing trajectories of Dota Auto Chess games using LDA and TSNE</title><link>https://gilgi.org/blog/autochess/</link><dc:creator>gilgi</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img src="https://gilgi.org/images/blog/autochess.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;The Dota 2 custom map "Dota Auto Chess" is taking the gaming world by storm. Auto Chess is almost like a card game in which players attempt to improve their hand round after round while their "chesses" (cards) battle the other players in the game automatically. In this post, we'll explore the potential of &lt;a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"&gt;latent Dirichlet allocation&lt;/a&gt; to model strategies (specific combinations of cards that are often played together) as "topics". We will then try to visualize the evolution of Auto Chess "hands" over the course of individual games in terms of their topics using &lt;a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"&gt;t-SNE&lt;/a&gt;. These kinds of visualizations might help us learn how to more effectively evolve our strategies.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1zlAkpsSBb6HN0Jqzfk842fPZlnbmxF2F"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gilgi.org/blog/autochess/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Dota 2</category><category>LDA</category><category>notebook</category><category>Python</category><category>TSNE</category><category>visualization</category><guid>https://gilgi.org/blog/autochess/</guid><pubDate>Sat, 18 May 2019 04:00:00 GMT</pubDate></item><item><title>Creating Dota 2 hero embeddings with Word2vec</title><link>https://gilgi.org/blog/dota-hero-embedding/</link><dc:creator>gilgi</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img src="https://gilgi.org/images/blog/dota-hero-embedding.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;One of the coolest results in natural language processing is the success of word embedding models like &lt;a href="https://en.wikipedia.org/wiki/Word2vec"&gt;Word2vec&lt;/a&gt;. These models are able to extract rich semantic information from words using surprisingly simple models like &lt;a href="https://en.wikipedia.org/wiki/Bag-of-words_model#CBOW"&gt;CBOW&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/N-gram#Skip-gram"&gt;skip-gram&lt;/a&gt;. What if we could use these generic modelling strategies to learn embeddings for something completely different - say, Dota 2 heroes.&lt;/p&gt;
&lt;p&gt;In this post, we'll use the &lt;a href="https://docs.opendota.com/"&gt;OpenDota API&lt;/a&gt; to collect data from professional Dota 2 matches and use &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; to train a Word2vec-like model for hero embeddings.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1V2BY-FkYRTEK6L335cBvGCkpvh8sU7_R/view?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gilgi.org/blog/dota-hero-embedding/"&gt;Read more…&lt;/a&gt; (17 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Dota 2</category><category>embedding</category><category>keras</category><category>machine learning</category><category>notebook</category><category>Python</category><category>Word2vec</category><guid>https://gilgi.org/blog/dota-hero-embedding/</guid><pubDate>Sun, 27 May 2018 04:00:00 GMT</pubDate></item></channel></rss>