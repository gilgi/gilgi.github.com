<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gilgi.org (Posts about machine learning)</title><link>https://gilgi.org/</link><description></description><atom:link href="https://gilgi.org/categories/machine-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:site@gilgi.org"&gt;gilgi&lt;/a&gt; </copyright><lastBuildDate>Sat, 23 May 2020 20:33:03 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Linear regression in Python (UPenn ENM 375 guest lecture)</title><link>https://gilgi.org/blog/linear-regression/</link><dc:creator>gilgi</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img src="https://gilgi.org/images/blog/linear-regression.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;I was recently invited to give a guest lecture in the course &lt;a href="https://catalog.upenn.edu/courses/enm/"&gt;ENM 375 Biological Data Science I - Fundamentals of Biostatistics&lt;/a&gt; at the University of Pennsylvania on the topic of &lt;a href="https://en.wikipedia.org/wiki/Linear_regression"&gt;linear regression&lt;/a&gt; in Python. As part of my lecture, I walked through this notebook. It might serve as a useful reference, covering everything from simulation and fitting to a wide variety of diagnostics. The walkthrough includes explanations of how to do everything in vanilla &lt;a href="https://numpy.org/"&gt;&lt;code&gt;numpy&lt;/code&gt;&lt;/a&gt;/&lt;a href="https://www.scipy.org/"&gt;&lt;code&gt;scipy&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://scikit-learn.org/"&gt;&lt;code&gt;scikit-learn&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://www.statsmodels.org/"&gt;&lt;code&gt;statsmodels&lt;/code&gt;&lt;/a&gt;. As a bonus, there's even a section on &lt;a href="https://en.wikipedia.org/wiki/Logistic_regression"&gt;logistic regression&lt;/a&gt; at the end.&lt;/p&gt;
&lt;p&gt;Read on for more!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/gilgi/gilgi.github.com/blob/src/posts/linear_regression.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gilgi.org/blog/linear-regression/"&gt;Read more…&lt;/a&gt; (19 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>machine learning</category><category>notebook</category><category>Python</category><category>statistics</category><guid>https://gilgi.org/blog/linear-regression/</guid><pubDate>Tue, 16 Apr 2019 04:00:00 GMT</pubDate></item><item><title>Stein's paradox</title><link>https://gilgi.org/blog/stein/</link><dc:creator>gilgi</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img src="https://gilgi.org/images/blog/stein.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;I recently heard of &lt;a href="https://en.wikipedia.org/wiki/Stein%27s_example"&gt;Stein's paradox&lt;/a&gt;, and at first I couldn't believe it! In this post, I'll convince myself by comparing the risk of a &lt;a href="https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator"&gt;James–Stein estimator&lt;/a&gt; to a naive estimator on a simulated high-dimensional dataset.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/gilgi/gilgi.github.com/blob/src/posts/stein.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gilgi.org/blog/stein/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>machine learning</category><category>notebook</category><category>Python</category><category>statistics</category><guid>https://gilgi.org/blog/stein/</guid><pubDate>Thu, 29 Nov 2018 05:00:00 GMT</pubDate></item><item><title>Creating Dota 2 hero embeddings with Word2vec</title><link>https://gilgi.org/blog/dota-hero-embedding/</link><dc:creator>gilgi</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img src="https://gilgi.org/images/blog/dota-hero-embedding.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;One of the coolest results in natural language processing is the success of word embedding models like &lt;a href="https://en.wikipedia.org/wiki/Word2vec"&gt;Word2vec&lt;/a&gt;. These models are able to extract rich semantic information from words using surprisingly simple models like &lt;a href="https://en.wikipedia.org/wiki/Bag-of-words_model#CBOW"&gt;CBOW&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/N-gram#Skip-gram"&gt;skip-gram&lt;/a&gt;. What if we could use these generic modelling strategies to learn embeddings for something completely different - say, Dota 2 heroes.&lt;/p&gt;
&lt;p&gt;In this post, we'll use the &lt;a href="https://docs.opendota.com/"&gt;OpenDota API&lt;/a&gt; to collect data from professional Dota 2 matches and use &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; to train a Word2vec-like model for hero embeddings.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/gilgi/gilgi.github.com/blob/src/posts/dota_hero_embedding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gilgi.org/blog/dota-hero-embedding/"&gt;Read more…&lt;/a&gt; (17 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Dota 2</category><category>embedding</category><category>keras</category><category>machine learning</category><category>notebook</category><category>Python</category><category>Word2vec</category><guid>https://gilgi.org/blog/dota-hero-embedding/</guid><pubDate>Sun, 27 May 2018 04:00:00 GMT</pubDate></item></channel></rss>